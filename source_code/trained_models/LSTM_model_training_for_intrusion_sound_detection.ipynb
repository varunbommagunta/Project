{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgZw6ZHc8sOG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Set the path to the directory containing the sound files\n",
        "sound_dir = \"/content/drive/MyDrive/Window Breaking/Window Breaking\"\n",
        "\n",
        "# Define the number of mel frequency bins in the spectrogram\n",
        "n_mels = 128\n",
        "\n",
        "# Define the number of time steps in each segment of the spectrogram\n",
        "n_steps = 128\n",
        "\n",
        "# Define the batch size and number of epochs for training\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "# Define a function to convert an audio file to a mel-spectrogram\n",
        "def file_to_melspec(filepath):\n",
        "    # Load the audio file and convert to mel-spectrogram\n",
        "    signal, sr = librosa.load(filepath, sr=22050)\n",
        "    spec = librosa.feature.melspectrogram(signal, sr=sr, n_mels=n_mels)\n",
        "    # Resize the spectrogram to n_steps x n_mels\n",
        "    spec = librosa.util.fix_length(spec, n_steps, axis=1)\n",
        "    # Convert to decibel scale\n",
        "    spec = librosa.power_to_db(spec, ref=np.max)\n",
        "    return spec\n",
        "\n",
        "# Load the sound files and labels into memory\n",
        "sound_files = []\n",
        "labels = []\n",
        "for label in os.listdir(sound_dir):\n",
        "    label_dir = os.path.join(sound_dir, label)\n",
        "    for filename in os.listdir(label_dir):\n",
        "        filepath = os.path.join(label_dir, filename)\n",
        "        sound_files.append(filepath)\n",
        "        labels.append(label)\n",
        "\n",
        "cnt = 0\n",
        "# Convert the sound files to mel-spectrograms and store in a numpy array\n",
        "specs = np.zeros((len(sound_files), n_mels, n_steps), dtype=np.float32)\n",
        "for i, filepath in enumerate(sound_files):\n",
        "    spec = file_to_melspec(filepath)\n",
        "    specs[i] = spec\n",
        "    print(cnt,end = \" \")\n",
        "    cnt +=1\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "labels = [label_map[label] for label in labels]\n",
        "labels = np.eye(len(label_map))[labels]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_specs, val_specs, train_labels, val_labels = train_test_split(specs, labels, test_size=0.2)\n",
        "\n",
        "# Define the RNN model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(n_mels, n_steps), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(label_map), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model on the training data and evaluate on the validation data\n",
        "model.fit(train_specs, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_specs, val_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('glass_sound_detection.h5')"
      ],
      "metadata": {
        "id": "2uvJJOxTBML7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to load an audio file and convert it to a mel-spectrogram\n",
        "def audio_file_to_melspec(audio_file):\n",
        "    # Load the audio file and convert to mel-spectrogram\n",
        "    signal, sr = librosa.load(audio_file, sr=22050)\n",
        "    spec = librosa.feature.melspectrogram(signal, sr=sr, n_mels=n_mels)\n",
        "    # Resize the spectrogram to n_steps x n_mels\n",
        "    spec = librosa.util.fix_length(spec, n_steps, axis=1)\n",
        "    # Convert to decibel scale\n",
        "    spec = librosa.power_to_db(spec, ref=np.max)\n",
        "    return spec\n",
        "\n",
        "# Load an audio file and convert it to a mel-spectrogram\n",
        "audio_file = '/content/sample_data/0_170.wav'\n",
        "spec = audio_file_to_melspec(audio_file)\n",
        "\n",
        "# Reshape the mel-spectrogram to match the input shape of the model\n",
        "spec = spec.reshape(1, n_mels, n_steps)\n",
        "\n",
        "# Make a prediction using the trained model\n",
        "prediction = model.predict(spec)\n",
        "\n",
        "print(prediction.tolist())\n",
        "\n",
        "# Print the predicted class label\n",
        "predicted_label = np.argmax(prediction)\n",
        "label_map = {i: label for label, i in label_map.items()}\n",
        "if predicted_label in label_map.keys():\n",
        "    print('Predicted class label:', label_map[predicted_label])\n",
        "else:\n",
        "    print('Unknown label')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQUieyBRD70u",
        "outputId": "8b79c840-4aaa-46b0-8f18-da007600d67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 53ms/step\n",
            "[[0.9998306035995483, 0.00016933951701503247]]\n",
            "Predicted class label: not glass sounds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEFJx2o1UV67",
        "outputId": "8d926469-1328-4df8-d16f-23322800958a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'not glass sounds', 1: 'glass sounds'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfR2XL8-UV5Y",
        "outputId": "81b3656e-1df6-462c-8426-0a05a42b92de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import sounddevice as sd\n",
        "\n",
        "\n",
        "# Set the path to the trained model\n",
        "model_path = '/content/glass_sound_detection.h5'\n",
        "\n",
        "# Set the number of mel frequency bins in the spectrogram\n",
        "n_mels = 128\n",
        "\n",
        "# Set the number of time steps in each segment of the spectrogram\n",
        "n_steps = 128\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Define a function to convert an audio array to a mel-spectrogram\n",
        "def array_to_melspec(audio):\n",
        "    # Convert to mel-spectrogram\n",
        "    spec = librosa.feature.melspectrogram(audio, sr=22050, n_mels=n_mels)\n",
        "    # Resize the spectrogram to n_steps x n_mels\n",
        "    spec = librosa.util.fix_length(spec, n_steps, axis=1)\n",
        "    # Convert to decibel scale\n",
        "    spec = librosa.power_to_db(spec, ref=np.max)\n",
        "    return spec\n",
        "\n",
        "# Define a function to record audio from the microphone\n",
        "def record(duration):\n",
        "    # Set the sample rate and number of channels\n",
        "    sr = 22050\n",
        "    channels = 1\n",
        "    # Record the audio\n",
        "    audio = sd.rec(int(duration * sr), samplerate=sr, channels=channels)\n",
        "    sd.wait()\n",
        "    # Convert to mono if necessary\n",
        "    if audio.ndim > 1:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "    return audio\n",
        "\n",
        "# Record 3 seconds of audio from the microphone\n",
        "duration = 3\n",
        "print('Recording...')\n",
        "audio = record(duration)\n",
        "\n",
        "# Convert the audio to a mel-spectrogram and make a prediction\n",
        "spec = array_to_melspec(audio)\n",
        "spec = np.expand_dims(spec, axis=0)\n",
        "prediction = model.predict(spec)\n",
        "label_map = {0: 'glass_breaking', 1: 'gunshot'}\n",
        "predicted_label = np.argmax(prediction)\n",
        "print('Predicted class label:', label_map[predicted_label])\n"
      ],
      "metadata": {
        "id": "4lzSwr1TGn39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5Nugb95bGohM",
        "outputId": "7d386950-ba8e-4f4d-9e37-a27fdad9c29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.11.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ik5ggmqQJiRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Set the path to the directory containing the sound files\n",
        "sound_dir = \"/content/drive/MyDrive/Window Breaking/Window Breaking\"\n",
        "\n",
        "# Define the number of mel frequency bins in the spectrogram\n",
        "n_mels = 128\n",
        "\n",
        "# Define the number of time steps in each segment of the spectrogram\n",
        "n_steps = 128\n",
        "\n",
        "# Define the batch size and number of epochs for training\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "# Define a function to convert an audio file to a mel-spectrogram\n",
        "def file_to_melspec(filepath):\n",
        "    # Load the audio file and convert to mel-spectrogram\n",
        "    signal, sr = librosa.load(filepath, sr=22050)\n",
        "    spec = librosa.feature.melspectrogram(signal, sr=sr, n_mels=n_mels)\n",
        "    # Resize the spectrogram to n_steps x n_mels\n",
        "    spec = librosa.util.fix_length(spec, n_steps, axis=1)\n",
        "    # Convert to decibel scale\n",
        "    spec = librosa.power_to_db(spec, ref=np.max)\n",
        "    return spec\n",
        "\n",
        "# Load the sound files and labels into memory\n",
        "sound_files = []\n",
        "labels = []\n",
        "for label in os.listdir(sound_dir):\n",
        "    label_dir = os.path.join(sound_dir, label)\n",
        "    for filename in os.listdir(label_dir):\n",
        "        filepath = os.path.join(label_dir, filename)\n",
        "        sound_files.append(filepath)\n",
        "        labels.append(label)\n",
        "\n",
        "cnt = 0\n",
        "# Convert the sound files to mel-spectrograms and store in a numpy array\n",
        "specs = np.zeros((len(sound_files), n_mels, n_steps), dtype=np.float32)\n",
        "for i, filepath in enumerate(sound_files):\n",
        "    spec = file_to_melspec(filepath)\n",
        "    specs[i] = spec\n",
        "    print(cnt,end = \" \")\n",
        "    cnt +=1\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "labels = [label_map[label] for label in labels]\n",
        "labels = np.eye(len(label_map))[labels]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_specs, val_specs, train_labels, val_labels = train_test_split(specs, labels, test_size=0.2)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(n_mels, n_steps, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(label_map), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model on the training data and evaluate on the validation data\n",
        "model.fit(train_specs, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_specs, val_labels))\n",
        "model.save('gsd_cnn.h5')\n"
      ],
      "metadata": {
        "id": "vUGs92CEXKyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(n_mels, n_steps, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(label_map), activation='softmax'))\n",
        "\n",
        "# Compile the model with categorical cross-entropy loss and evaluation metrics\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=5, mode='max')\n",
        "\n",
        "# Train the model on the training data and evaluate on the validation data\n",
        "model.fit(train_specs, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_specs, val_labels), callbacks=[early_stop])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_t2eTDPah4D",
        "outputId": "0d7ce948-dbcf-4d8a-a391-603ba91a26f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "31/31 [==============================] - 31s 955ms/step - loss: 16.9376 - accuracy: 0.7686 - val_loss: 1.0990 - val_accuracy: 0.9917\n",
            "Epoch 2/10\n",
            "31/31 [==============================] - 28s 903ms/step - loss: 1.1325 - accuracy: 0.9700 - val_loss: 1.0386 - val_accuracy: 0.9628\n",
            "Epoch 3/10\n",
            "31/31 [==============================] - 29s 947ms/step - loss: 1.0691 - accuracy: 0.9628 - val_loss: 1.1059 - val_accuracy: 0.9835\n",
            "Epoch 4/10\n",
            "31/31 [==============================] - 30s 972ms/step - loss: 0.9798 - accuracy: 0.9835 - val_loss: 0.8657 - val_accuracy: 0.9876\n",
            "Epoch 5/10\n",
            "31/31 [==============================] - 29s 949ms/step - loss: 0.8963 - accuracy: 0.9814 - val_loss: 0.8332 - val_accuracy: 0.9876\n",
            "Epoch 6/10\n",
            "31/31 [==============================] - 31s 1s/step - loss: 0.8379 - accuracy: 0.9897 - val_loss: 0.7808 - val_accuracy: 0.9959\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f462a440190>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "# Define a function to load an audio file and convert it to a mel-spectrogram\n",
        "def audio_file_to_melspec(audio_file):\n",
        "    # Load the audio file and convert to mel-spectrogram\n",
        "    signal, sr = librosa.load(audio_file, sr=22050)\n",
        "    spec = librosa.feature.melspectrogram(signal, sr=sr, n_mels=n_mels)\n",
        "    # Resize the spectrogram to n_steps x n_mels\n",
        "    spec = librosa.util.fix_length(spec, n_steps, axis=1)\n",
        "    # Convert to decibel scale\n",
        "    spec = librosa.power_to_db(spec, ref=np.max)\n",
        "    return spec\n",
        "\n",
        "# Load an audio file and convert it to a mel-spectrogram\n",
        "audio_file = '/content/sample_data/segment_625.mp3'\n",
        "spec = audio_file_to_melspec(audio_file)\n",
        "\n",
        "# Reshape the mel-spectrogram to match the input shape of the model\n",
        "spec = spec.reshape(1, n_mels, n_steps)\n",
        "\n",
        "# Make a prediction using the trained model\n",
        "model_path = \"/content/glass_sound_detection.h5\"\n",
        "model = load_model(model_path)\n",
        "prediction = model.predict(spec)\n",
        "\n",
        "print(prediction.tolist())\n",
        "\n",
        "# Print the predicted class label\n",
        "predicted_label = np.argmax(prediction)\n",
        "label_map = {i: label for label, i in label_map.items()}\n",
        "if predicted_label in label_map.keys():\n",
        "    print('Predicted class label:', label_map[predicted_label])\n",
        "else:\n",
        "    print('Unknown label')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nsh9X8UvXKwu",
        "outputId": "2312d854-4a50-46b0-eb32-7da5482b66a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 34 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f462951ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 496ms/step\n",
            "[[1.6284376513908683e-08, 1.0]]\n",
            "Predicted class label: glass sounds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BtJLqvKmaQj4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}